{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee4b218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 13:10:47.320717: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-18 13:10:47.321455: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-18 13:10:47.324625: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-18 13:10:47.332849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752862247.346700   19595 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752862247.350471   19595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752862247.360555   19595 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752862247.360581   19595 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752862247.360583   19595 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752862247.360584   19595 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-18 13:10:47.364088: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import tensorflow as tf \n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "EPOCHS = 32\n",
    "BATCH_SIZE = 256\n",
    "INPUT_FILE_NAME = '../data/frankenstein.txt'\n",
    "WINDOW_LENGHT = 40\n",
    "WINDOW_STEP = 3\n",
    "BEAM_SIZE = 8\n",
    "NUM_LETTERS = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(INPUT_FILE_NAME, 'r', encoding= \"utf-8-sig\",)\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "text = text.lower()\n",
    "text = text.replace('\\n', '')\n",
    "text = text.replace('  ', ' ')\n",
    "\n",
    "unique_charts = list(set(text))\n",
    "char_to_index = dict((ch,index) for index, ch in enumerate(unique_charts))\n",
    "index_to_chat = dict((index,ch) for index, ch in enumerate(unique_charts))\n",
    "encoding_width = len(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e04d4bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = []\n",
    "targets = []\n",
    "for i in range(0,len(text) - WINDOW_LENGHT, WINDOW_STEP):\n",
    "    fragments.append(text[i: i + WINDOW_LENGHT])\n",
    "    targets.append(text[i + WINDOW_LENGHT])\n",
    "\n",
    "X = np.zeros((len(fragments),WINDOW_LENGHT,encoding_width))\n",
    "y = np.zeros((len(fragments), encoding_width))\n",
    "\n",
    "for i, fragment in enumerate(fragments):\n",
    "    for j, char in enumerate(fragment):\n",
    "        X[i,j, char_to_index[char]] = 1\n",
    "    target_char = targets[i]\n",
    "    y[i,char_to_index[target_char]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f0950c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a10feb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "533/533 - 148s - 278ms/step - loss: 2.8197 - val_loss: 2.7427\n",
      "Epoch 2/32\n",
      "533/533 - 131s - 246ms/step - loss: 2.4520 - val_loss: 2.6121\n",
      "Epoch 3/32\n",
      "533/533 - 123s - 230ms/step - loss: 2.3322 - val_loss: 2.5430\n",
      "Epoch 4/32\n",
      "533/533 - 119s - 223ms/step - loss: 2.2494 - val_loss: 2.4853\n",
      "Epoch 5/32\n",
      "533/533 - 120s - 225ms/step - loss: 2.1859 - val_loss: 2.4496\n",
      "Epoch 6/32\n",
      "533/533 - 118s - 221ms/step - loss: 2.1360 - val_loss: 2.4180\n",
      "Epoch 7/32\n",
      "533/533 - 117s - 220ms/step - loss: 2.0939 - val_loss: 2.3864\n",
      "Epoch 8/32\n",
      "533/533 - 118s - 222ms/step - loss: 2.0591 - val_loss: 2.3683\n",
      "Epoch 9/32\n",
      "533/533 - 119s - 223ms/step - loss: 2.0256 - val_loss: 2.3489\n",
      "Epoch 10/32\n",
      "533/533 - 120s - 224ms/step - loss: 1.9966 - val_loss: 2.3348\n",
      "Epoch 11/32\n",
      "533/533 - 120s - 225ms/step - loss: 1.9760 - val_loss: 2.3234\n",
      "Epoch 12/32\n",
      "533/533 - 118s - 222ms/step - loss: 1.9524 - val_loss: 2.3078\n",
      "Epoch 13/32\n",
      "533/533 - 117s - 220ms/step - loss: 1.9325 - val_loss: 2.3057\n",
      "Epoch 14/32\n",
      "533/533 - 119s - 223ms/step - loss: 1.9152 - val_loss: 2.2968\n",
      "Epoch 15/32\n",
      "533/533 - 117s - 220ms/step - loss: 1.8948 - val_loss: 2.2838\n",
      "Epoch 16/32\n",
      "533/533 - 117s - 219ms/step - loss: 1.8796 - val_loss: 2.2848\n",
      "Epoch 17/32\n",
      "533/533 - 117s - 220ms/step - loss: 1.8664 - val_loss: 2.2750\n",
      "Epoch 18/32\n",
      "533/533 - 120s - 225ms/step - loss: 1.8502 - val_loss: 2.2741\n",
      "Epoch 19/32\n",
      "533/533 - 117s - 220ms/step - loss: 1.8380 - val_loss: 2.2667\n",
      "Epoch 20/32\n",
      "533/533 - 118s - 221ms/step - loss: 1.8278 - val_loss: 2.2777\n",
      "Epoch 21/32\n",
      "533/533 - 116s - 219ms/step - loss: 1.8160 - val_loss: 2.2655\n",
      "Epoch 22/32\n",
      "533/533 - 118s - 221ms/step - loss: 1.8052 - val_loss: 2.2519\n",
      "Epoch 23/32\n",
      "533/533 - 117s - 219ms/step - loss: 1.7940 - val_loss: 2.2455\n",
      "Epoch 24/32\n",
      "533/533 - 116s - 218ms/step - loss: 1.7851 - val_loss: 2.2560\n",
      "Epoch 25/32\n",
      "533/533 - 116s - 218ms/step - loss: 1.7757 - val_loss: 2.2553\n",
      "Epoch 26/32\n",
      "533/533 - 117s - 220ms/step - loss: 1.7679 - val_loss: 2.2500\n",
      "Epoch 27/32\n",
      "533/533 - 117s - 219ms/step - loss: 1.7578 - val_loss: 2.2450\n",
      "Epoch 28/32\n",
      "533/533 - 119s - 224ms/step - loss: 1.7499 - val_loss: 2.2510\n",
      "Epoch 29/32\n",
      "533/533 - 124s - 232ms/step - loss: 1.7446 - val_loss: 2.2564\n",
      "Epoch 30/32\n",
      "533/533 - 125s - 234ms/step - loss: 1.7359 - val_loss: 2.2467\n",
      "Epoch 31/32\n",
      "533/533 - 124s - 233ms/step - loss: 1.7273 - val_loss: 2.2428\n",
      "Epoch 32/32\n",
      "533/533 - 123s - 232ms/step - loss: 1.7241 - val_loss: 2.2367\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, dropout = 0.2, recurrent_dropout= 0.2, input_shape = (None, encoding_width)))\n",
    "model.add(LSTM(128,dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(encoding_width, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "history = model.fit(X,y, validation_split = 0.05, batch_size = BATCH_SIZE, epochs = EPOCHS, verbose = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67f6ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the body which i had been that i had been the father, and the father, and the s\n",
      "the body which i had been the father, and the father, and the father, and the s\n",
      "the body which i had been that i had been the father, and the father, and the f\n",
      "the body which i had been that i had been the father, and the father, and the m\n",
      "the body which i had been the father, and the father, and the father, and the f\n",
      "the body which i had been that i had been the father, and the father of the cou\n",
      "the body which i had been the father, and the father, and the father, and the m\n",
      "the body which i had been that i had been the father, and the father, and the c\n"
     ]
    }
   ],
   "source": [
    "letters = \"the body \"\n",
    "one_hots = []\n",
    "for i, char in enumerate(letters):\n",
    "    x = np.zeros(encoding_width)\n",
    "    x[char_to_index[char]] = 1\n",
    "    one_hots.append(x)\n",
    "beams = [(np.log(1.0),letters, one_hots)]\n",
    "\n",
    "for i in range(70):\n",
    "    minibatch_list = []\n",
    "\n",
    "    for triple in beams: \n",
    "        minibatch_list.append(triple[2])\n",
    "    minibatch = np.array(minibatch_list)\n",
    "    y_predict = model.predict(minibatch, verbose = 0)\n",
    "    new_beams = []\n",
    "    for j , softmax_vec in enumerate(y_predict):\n",
    "        triple = beams[j]\n",
    "        for k in range(BEAM_SIZE):\n",
    "            char_index = np.argmax(softmax_vec)\n",
    "            new_prob = triple[0] + np.log(softmax_vec[char_index])\n",
    "            new_letters = triple[1] + index_to_chat[char_index]\n",
    "            x = np.zeros(encoding_width)\n",
    "            x[char_index] = 1\n",
    "            new_one_hots = triple[2].copy()\n",
    "            new_one_hots.append(x)\n",
    "            new_beams.append((new_prob,new_letters,new_one_hots))\n",
    "            softmax_vec[char_index] = 0\n",
    "    new_beams.sort(key=lambda tup: tup[0], reverse= True)\n",
    "    beams = new_beams[0:BEAM_SIZE]\n",
    "for item in beams:\n",
    "    print(item[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e6e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the body ooooooooooo\n",
      "the body *oooooooooo\n",
      "the body ]oooooooooo\n",
      "the body uoooooooooo\n",
      "the body 9oooooooooo\n",
      "the body o*ooooooooo\n",
      "the body yoooooooooo\n",
      "the body 5oooooooooo\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "letters = \"the body \"\n",
    "one_hots = []\n",
    "for char in letters:\n",
    "    x = np.zeros(encoding_width)\n",
    "    x[char_to_index[char]] = 1\n",
    "    one_hots.append(x)\n",
    "\n",
    "beams = [(np.log(1.0), letters, one_hots)]\n",
    "\n",
    "for _ in range(NUM_LETTERS):\n",
    "    minibatch_list = [beam[2] for beam in beams]\n",
    "    minibatch = np.array(minibatch_list)  # shape: (beam_size, seq_len, encoding_width)\n",
    "    \n",
    "    y_predict = model.predict(minibatch, verbose=0)\n",
    "    new_beams = []\n",
    "\n",
    "    for j, softmax_vec in enumerate(y_predict):\n",
    "        log_prob, sequence, one_hot_seq = beams[j]\n",
    "        top_k_indices = np.argsort(softmax_vec)[-BEAM_SIZE:][::-1]\n",
    "\n",
    "        for idx in top_k_indices:\n",
    "            prob = softmax_vec[idx]\n",
    "            if prob == 0:  # Avoid log(0)\n",
    "                continue\n",
    "            new_log_prob = log_prob + np.log(prob)\n",
    "            new_sequence = sequence + index_to_char[idx]\n",
    "            x = np.zeros(encoding_width)\n",
    "            x[idx] = 1\n",
    "            new_one_hot_seq = [vec.copy() for vec in one_hot_seq]\n",
    "            new_one_hot_seq.append(x)\n",
    "            new_beams.append((new_log_prob, new_sequence, new_one_hot_seq))\n",
    "\n",
    "    new_beams.sort(key=lambda tup: tup[0], reverse=True)\n",
    "    beams = new_beams[:BEAM_SIZE]\n",
    "\n",
    "for log_prob, text, _ in beams:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208597d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequence = \"the body\"\n",
    "one_hots = []\n",
    "for char in sequence:\n",
    "    x = np.zeros(encoding_width)\n",
    "    x[char_to_index[char]] = 1\n",
    "    one_hots.append(x)\n",
    "\n",
    "input_sequence = np.array([one_hots]) \n",
    "y_pred = model.predict(input_sequence, verbose=0)[0]  \n",
    "\n",
    "predicted_index = np.argmax(y_pred)\n",
    "predicted_char = index_to_char[predicted_index]\n",
    "\n",
    "print(\"Siguiente letra predicha:\", predicted_char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
