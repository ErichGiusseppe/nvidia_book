{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50db5d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 13:08:46.798127: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-10 13:08:46.805660: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-10 13:08:46.863158: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-10 13:08:46.915723: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752170926.959879   93763 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752170926.974033   93763 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752170927.068890   93763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752170927.068912   93763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752170927.068913   93763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752170927.068914   93763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-10 13:08:47.080291: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4bff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c38485",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603d3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(train_images)\n",
    "stddev = np.std(train_images)\n",
    "train_images = (train_images - mean) / stddev\n",
    "test_images = (test_images - mean) / stddev\n",
    "train_labels = to_categorical(train_labels, num_classes=10)\n",
    "test_labels = to_categorical(test_labels, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb915d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erich/dev/nvidia_book/.venv/lib/python3.11/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 - 64s - 1ms/step - accuracy: 0.6602 - loss: 0.0534 - val_accuracy: 0.8931 - val_loss: 0.0250\n",
      "Epoch 2/20\n",
      "60000/60000 - 93s - 2ms/step - accuracy: 0.9006 - loss: 0.0202 - val_accuracy: 0.9171 - val_loss: 0.0165\n",
      "Epoch 3/20\n",
      "60000/60000 - 93s - 2ms/step - accuracy: 0.9154 - loss: 0.0157 - val_accuracy: 0.9261 - val_loss: 0.0142\n",
      "Epoch 4/20\n",
      "60000/60000 - 92s - 2ms/step - accuracy: 0.9231 - loss: 0.0139 - val_accuracy: 0.9291 - val_loss: 0.0131\n",
      "Epoch 5/20\n",
      "60000/60000 - 94s - 2ms/step - accuracy: 0.9276 - loss: 0.0128 - val_accuracy: 0.9315 - val_loss: 0.0124\n",
      "Epoch 6/20\n",
      "60000/60000 - 92s - 2ms/step - accuracy: 0.9313 - loss: 0.0121 - val_accuracy: 0.9333 - val_loss: 0.0119\n",
      "Epoch 7/20\n",
      "60000/60000 - 94s - 2ms/step - accuracy: 0.9339 - loss: 0.0116 - val_accuracy: 0.9346 - val_loss: 0.0116\n",
      "Epoch 8/20\n",
      "60000/60000 - 92s - 2ms/step - accuracy: 0.9363 - loss: 0.0111 - val_accuracy: 0.9362 - val_loss: 0.0113\n",
      "Epoch 9/20\n",
      "60000/60000 - 92s - 2ms/step - accuracy: 0.9384 - loss: 0.0108 - val_accuracy: 0.9377 - val_loss: 0.0110\n",
      "Epoch 10/20\n",
      "60000/60000 - 93s - 2ms/step - accuracy: 0.9401 - loss: 0.0104 - val_accuracy: 0.9387 - val_loss: 0.0108\n",
      "Epoch 11/20\n",
      "60000/60000 - 93s - 2ms/step - accuracy: 0.9417 - loss: 0.0102 - val_accuracy: 0.9390 - val_loss: 0.0107\n",
      "Epoch 12/20\n",
      "60000/60000 - 92s - 2ms/step - accuracy: 0.9434 - loss: 0.0099 - val_accuracy: 0.9396 - val_loss: 0.0105\n",
      "Epoch 13/20\n",
      "60000/60000 - 94s - 2ms/step - accuracy: 0.9445 - loss: 0.0097 - val_accuracy: 0.9397 - val_loss: 0.0104\n",
      "Epoch 14/20\n",
      "60000/60000 - 94s - 2ms/step - accuracy: 0.9456 - loss: 0.0095 - val_accuracy: 0.9397 - val_loss: 0.0103\n",
      "Epoch 15/20\n",
      "60000/60000 - 93s - 2ms/step - accuracy: 0.9466 - loss: 0.0094 - val_accuracy: 0.9403 - val_loss: 0.0102\n",
      "Epoch 16/20\n",
      "60000/60000 - 94s - 2ms/step - accuracy: 0.9475 - loss: 0.0092 - val_accuracy: 0.9401 - val_loss: 0.0102\n",
      "Epoch 17/20\n",
      "60000/60000 - 93s - 2ms/step - accuracy: 0.9483 - loss: 0.0091 - val_accuracy: 0.9402 - val_loss: 0.0101\n",
      "Epoch 18/20\n",
      "60000/60000 - 94s - 2ms/step - accuracy: 0.9491 - loss: 0.0089 - val_accuracy: 0.9408 - val_loss: 0.0100\n",
      "Epoch 19/20\n",
      "60000/60000 - 93s - 2ms/step - accuracy: 0.9496 - loss: 0.0088 - val_accuracy: 0.9406 - val_loss: 0.0100\n",
      "Epoch 20/20\n",
      "60000/60000 - 94s - 2ms/step - accuracy: 0.9503 - loss: 0.0087 - val_accuracy: 0.9407 - val_loss: 0.0099\n"
     ]
    }
   ],
   "source": [
    "initializer = keras.initializers.RandomUniform(minval=-0.1, maxval=0.1)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(\n",
    "            25,\n",
    "            activation=\"tanh\",\n",
    "            kernel_initializer=initializer,\n",
    "            bias_initializer=\"zeros\",\n",
    "        ),\n",
    "        keras.layers.Dense(\n",
    "            10,\n",
    "            activation=\"sigmoid\",\n",
    "            kernel_initializer=initializer,\n",
    "            bias_initializer=\"zeros\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaration of diferentes initializers\n",
    "from tensorflow.keras.initializers import (\n",
    "    glorot_uniform,\n",
    "    glorot_normal,\n",
    "    GlorotUniform,\n",
    "    GlorotNormal,\n",
    "    he_uniform,\n",
    "    he_normal,\n",
    "    HeUniform,\n",
    "    HeNormal,\n",
    ")\n",
    "\n",
    "# Mejor para tanh y sigmoid\n",
    "# Alias - distribución uniforme\n",
    "initializer = glorot_uniform()\n",
    "\n",
    "# Clase - distribución uniforme\n",
    "initializer = GlorotUniform(seed=None)\n",
    "\n",
    "# Alias - distribución normal\n",
    "initializer = glorot_normal()\n",
    "\n",
    "# Clase - distribución normal\n",
    "initializer = GlorotNormal(seed=None)\n",
    "\n",
    "# mejor para ReLU, LeakyReLU, etc.\n",
    "# Alias - distribución uniforme\n",
    "initializer = he_uniform()\n",
    "\n",
    "# Clase - distribución uniforme\n",
    "initializer = HeUniform(seed=None)\n",
    "\n",
    "# Alias - distribución normal\n",
    "initializer = he_normal()\n",
    "\n",
    "# Clase - distribución normal\n",
    "initializer = HeNormal(seed=None)\n",
    "\n",
    "# En las publicaciones se muestra, uniforme con glorot y normal con He"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fdc979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de inicializacion si no se necesita hacerles ajustes especificos a los inicializadores.\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(25, activation=\"tanh\", kernel_initializer=\"glorot_normal\"),\n",
    "        keras.layers.Dense(\n",
    "            10,\n",
    "            activation=\"sigmoid\",\n",
    "            kernel_initializer=\"glorot_uniform\",\n",
    "            bias_initializer=\"zeros\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed965e5",
   "metadata": {},
   "source": [
    "### INPUT STANDARIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d221c908",
   "metadata": {},
   "source": [
    "Aunque las redes neuronales tienen tendencia a ser bastante efectivas a la hora de captar diferentes distribuciones, es bueno estandarizar para tener los valores cercanos al 0 y por ende reducir el riesgo de saturar las neuronas desde el inicio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce74827",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d050de",
   "metadata": {},
   "source": [
    "Este concepto trata de normalizar los outputs de ciertas partes de las arquitecturas para poder, disminuir la probabilidad de sobre saturar a las neuronas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626805ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduced in 2015 by Szegedy\n",
    "keras.layers.Dense(64)\n",
    "keras.layers.BatchNormalization()\n",
    "keras.layers.Activarion('tanh')\n",
    "\n",
    "# the batch normalization also works well after the activation funcion \n",
    "keras.layers.Dense(64,activation='tanh')\n",
    "keras.layers.BatchNormalization()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
